{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oskar Lundqvist; osauli-0@student.ltu.se\n",
    "\n",
    "Filip Renberg; filren-0@student.ltu.se\n",
    "\n",
    "We are gonna do a simple lab where we train a model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use the mnist dataset of digits imported from keras like we did in lab 1 and lab 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\oskar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "input_shape = (28,28,1)\n",
    "num_classes = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape:  (60000, 28, 28, 1)\n",
      "60000 number of training samples\n",
      "10000 number of testing samples\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "#normalize input values from RGB to Greyscale [0,1]\n",
    "train_data = train_data.astype(\"float32\")/255\n",
    "test_data = test_data.astype(\"float32\")/255\n",
    "#Make sure images have the correct shape(28,28,1)\n",
    "train_data = np.expand_dims(train_data, -1)\n",
    "test_data = np.expand_dims(test_data, -1)\n",
    "\n",
    "#converts class vectors to binary class matrices\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "print(\"training shape: \", train_data.shape)\n",
    "print(train_data.shape[0], \"number of training samples\")\n",
    "print(test_data.shape[0], \"number of testing samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our multi-layer neural network using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n = 1, nr_filter = 32, kernel_size = (3, 3)):\n",
    "    # n = the number of conv2d layers\n",
    "    hidden_layers = []\n",
    "\n",
    "    hidden_layers.append(keras.Input(shape=input_shape)) # input layer\n",
    "\n",
    "    for i in range(n):\n",
    "        hidden_layers.append(layers.Conv2D(nr_filter, kernel_size=kernel_size, activation=\"relu\"))\n",
    "        hidden_layers.append(layers.MaxPooling2D(pool_size=[2,2]))\n",
    "\n",
    "        nr_filter*=2\n",
    "\n",
    "    hidden_layers.append(layers.Flatten())\n",
    "    hidden_layers.append(layers.Dropout(0.5)) #prevents overfitting\n",
    "    hidden_layers.append(layers.Dense(num_classes, activation=\"softmax\"))\n",
    "\n",
    "    model = keras.Sequential(hidden_layers)\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are training the model. We can use several different loss functions here, for this project we will use \"categorical_crossentropy\", \"sparse_categorical_crossentropy\" and \"binary_crossentropy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "size_batch = 128\n",
    "\n",
    "def model_training(pick, model):\n",
    "    match pick:\n",
    "        case 1:\n",
    "            #categorical loss\n",
    "            model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(train_data, train_labels, batch_size=size_batch, epochs=n_epochs, validation_split=0.1, verbose=0)\n",
    "        case 2:\n",
    "            #sparse categorical loss\n",
    "            model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(train_data, train_labels, batch_size=size_batch, epochs=n_epochs, validation_split=0.1, verbose=0)\n",
    "        case 3:\n",
    "            #binary loss\n",
    "            model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "            model.fit(train_data, train_labels, batch_size=size_batch, epochs=n_epochs, validation_split=0.1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93962 (367.04 KB)\n",
      "Trainable params: 93962 (367.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'History' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m model_training(\u001b[38;5;241m1\u001b[39m, model)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#evaluate\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m(train_data, train_labels, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, score[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'History' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "# See this as an example for how we define a test case\n",
    "\n",
    "# Model with 3 conv2d layers, using categorical_crossentropy loss function.\n",
    "model = create_model(n=3, nr_filter=12, kernel_size=(3, 3))\n",
    "train = model_training(1, model)\n",
    "#evaluate\n",
    "score = model.evaluate(train_data, train_labels, verbose=0)\n",
    "print(\"accuracy: \\t\", score[1])\n",
    "print(\"loss: \\t\\t\", score[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
